{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A *fast* one hot encoder with sklearn and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've worked in data science for any length of time, you've undoubtedly transformed your data into a one hot encoded format before. In this post we'll explore implementing a *fast* one hot encoder with  [scikit-learn](http://scikit-learn.org/stable/) and [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn's one hot encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` has implemented several classes for one hot encoding data from various formats (`DictVectorizer`, `OneHotEncoder` and `CategoricalEncoder` - not in current release). In this post we'll compare our implementation to `DictVectorizer` which is the most natural for working with `pandas.DataFrame`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pros of DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DictVectorizer` has the following great features which we will preserve in our implementation\n",
    "\n",
    "1. Works great in `sklearn` pipelines and train/test splits.\n",
    "    - If feature was present during train time but not in the data at predict time\n",
    "      `DictVectorizer` will automatically set the corresponding value to 0.\n",
    "    - If a feature is present at predict time that was not at train time it is not\n",
    "      encoded.\n",
    "2. Features to be encoded are inferred from the data (user does not need to specify this).\n",
    "    - This means numeric values in the input remain unchanged and `str` fields are\n",
    "      encoded automatically.\n",
    "3. We can get a mapping from feature names to the one hot encoded transformation values.\n",
    "    - This is useful for looking at coefficients of feature importances of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cons of DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DictVectorizer` has two main blemishes (as related to a specific but common use case, see disclaimer below).\n",
    "\n",
    "1. Transforming large `DataFrame`s is **slow**\n",
    "2. The `.fit()` and `.transform()` signatures do not accept `DataFrame`s. To use `DictVectorizer`\n",
    "    a `DataFrame` must first be converted to a `list` of `dict`s (which is also slow), e.g.\n",
    "    \n",
    "```python\n",
    "    DictVectorizer().fit_transform(X.to_dict('records'))\n",
    "```\n",
    "    \n",
    "Our implementation will guarantee the features of `DictVectorizer` listed in the pros section above and improve the conds by accepting a `DataFrame` as input and vastly improving the speed of the transformation. Our implementation will get a boost in performance by wrapping the super fast `pandas.get_dummies()` with a subclass of `sklearn.base.TransformerMixin`.\n",
    "\n",
    "Before we get started let's compare the speed of `DictVectorizer` with `pandas.get_dummies()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An improved one hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our improved implementation will mimic the `DictVectorizer` interface (except that it accepts `DataFrame`s as input) by wrapping the super fast `pandas.get_dummies()` with a subclass of `sklearn.base.TransformerMixin`. Subclassing the `TransformerMixin` makes it easy for our class to integrate with popular `sklearn` paradigms such as their `Pipeline`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are specifically comparing the speed of `DictVectorizer` for the following use case only.\n",
    "\n",
    "1. We are starting with a `DataFrame` which must be converted to a list of `dict`s\n",
    "2. We are only interested in dense output, e.g. `DictVectorizer(sparse=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started let's compare the speed of `DictVectorizer` with `pandas.get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create *large* data set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SIZE = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'int1': np.random.randint(0, 100, size=SIZE),\n",
    "    'int2': np.random.randint(0, 100, size=SIZE),\n",
    "    'float1': np.random.uniform(size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(10)], size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(75)], size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(150)], size=SIZE),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 7.16 ms, total: 18.9 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `pandas.get_dummies()` is fast. Let's take a look at `DictVectorizer`s speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 407 ms, sys: 101 ms, total: 508 ms\n",
      "Wall time: 759 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "_ = DictVectorizer(sparse=False).fit_transform(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also informative to see that although `DictVectorizer` is slower than `pandas.get_dummies()`, converting the `DataFrame` to a `list` of `dict`s is the real bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 6.82 ms, total: 191 ms\n",
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time just to get list of dicts\n",
    "df_dicts = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.1 ms, sys: 7.27 ms, total: 62.3 ms\n",
      "Wall time: 92.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time to transform dicts\n",
    "_ = DictVectorizer(sparse=False).fit_transform(df_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see `pandas.get_dummies()` is *much* faster for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "class GetDummies(sklearn.base.TransformerMixin):\n",
    "    \"\"\"Fast one-hot-encoder that makes use of pandas.get_dummies() safely\n",
    "    on train/test splits.\n",
    "    \"\"\"\n",
    "    def __init__(self, dtypes=None):\n",
    "        self.input_columns = None\n",
    "        self.final_columns = None\n",
    "        if dtypes is None:\n",
    "            dtypes = [object, 'category']\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.input_columns = list(X.select_dtypes(self.dtypes).columns)\n",
    "        X = pd.get_dummies(X, columns=self.input_columns)\n",
    "        self.final_columns = X.columns\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        X = pd.get_dummies(X, columns=self.input_columns)\n",
    "        X_columns = X.columns\n",
    "        # if columns in X had values not in the data set used during\n",
    "        # fit add them and set to 0\n",
    "        missing = set(self.final_columns) - set(X_columns)\n",
    "        for c in missing:\n",
    "            X[c] = 0\n",
    "        # remove any new columns that may have resulted from values in\n",
    "        # X that were not in the data set when fit\n",
    "        return X[self.final_columns]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return tuple(self.final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 702 Âµs, total: 14.7 ms\n",
      "Wall time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# let's take a look at its speed\n",
    "get_dummies = GetDummies()\n",
    "get_dummies.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the GetDummies implentation has slowed down a bit from the original `pandas.get_dummes()` due to the overhead of making sure it handles train/test splits correctly, however its still super fast.\n",
    "\n",
    "Let's also take a look at some of its other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.1 ms, sys: 22.3 ms, total: 74.4 ms\n",
      "Wall time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it works in sklearn pipelines too\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(\n",
    "    GetDummies(),\n",
    "    DecisionTreeClassifier(max_depth=3)\n",
    ")\n",
    "model.fit(df, np.random.choice([0, 1], size=SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature str1_47 (0.235331)\n",
      "2. feature int2 (0.232977)\n",
      "3. feature float1 (0.218621)\n",
      "4. feature str1_118 (0.157708)\n",
      "5. feature str1_97 (0.155362)\n",
      "6. feature str1_131 (0.000000)\n",
      "7. feature str1_144 (0.000000)\n",
      "8. feature str1_143 (0.000000)\n",
      "9. feature str1_142 (0.000000)\n",
      "10. feature str1_141 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# you can also pull out the feature names to look at feature importances\n",
    "\n",
    "tree = model.steps[-1][-1]\n",
    "importances = tree.feature_importances_\n",
    "std = np.std(tree.feature_importances_)\n",
    "\n",
    "indices = np.argsort(importances)[:10:-1]\n",
    "feature_names = model.steps[0][-1].get_feature_names()\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(len(feature_names[:10])):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bar_a</th>\n",
       "      <th>bar_c</th>\n",
       "      <th>baz_b</th>\n",
       "      <th>baz_d</th>\n",
       "      <th>grok_hi</th>\n",
       "      <th>grok_there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   foo  bar_a  bar_c  baz_b  baz_d  grok_hi  grok_there\n",
       "0    1      1      0      1      0        1           0\n",
       "1    2      0      1      0      1        0           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/test splits are safe!\n",
    "get_dummies = GetDummies()\n",
    "\n",
    "# create test data that demonstrates how GetDummies handles\n",
    "# different train/test conditions\n",
    "df1 = pd.DataFrame([\n",
    "    [1, 'a', 'b', 'hi'],\n",
    "    [2, 'c', 'd', 'there']\n",
    "], columns=['foo', 'bar', 'baz', 'grok'])\n",
    "df2 = pd.DataFrame([\n",
    "    [3, 'a', 'e', 'whoa', 0],\n",
    "    [4, 'c', 'b', 'whoa', 0],\n",
    "    [4, 'c', 'b', 'there', 0],\n",
    "], columns=['foo', 'bar', 'baz', 'grok', 'new'])\n",
    "\n",
    "get_dummies.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bar_a</th>\n",
       "      <th>bar_c</th>\n",
       "      <th>baz_b</th>\n",
       "      <th>baz_d</th>\n",
       "      <th>grok_hi</th>\n",
       "      <th>grok_there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   foo  bar_a  bar_c  baz_b  baz_d  grok_hi  grok_there\n",
       "0    3      1      0      0      0        0           0\n",
       "1    4      0      1      1      0        0           0\n",
       "2    4      0      1      1      0        0           1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. the new values of 'e' and 'whoa' are not encoded\n",
    "# 2. features baz_b and baz_d are both set to 0 when no suitable\n",
    "#      value for baz is found\n",
    "get_dummies.transform(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend using the `GetDummies` class as needed only. `sklearn` is a true industry standard and deviations thereof should occur only be when the size of the data necessitates such a change. For most modest sized data sets `DictVectorizer` has served me well. However, I've also reduced the time required to one hot encode my data from ~30 minutes to ~5 on data sets of around 100M records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
