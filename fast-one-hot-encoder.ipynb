{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A *fast* one hot encoder with sklearn and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've worked in data science for any length of time, you've undoubtedly transformed your data into a one hot encoded format before. In this post we'll explore implementing a *fast* one hot encoder with  [scikit-learn](http://scikit-learn.org/stable/) and [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn's one hot encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` has implemented several classes for one hot encoding data from various formats (`DictVectorizer`, `OneHotEncoder` and `CategoricalEncoder` - not in current release). In this post we'll compare our implementation to `DictVectorizer` which is the most natural for working with `pandas.DataFrames`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pros of DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DictVectorizer` has the following great features which we will preserve in our implementation\n",
    "\n",
    "1. Works great in `sklearn` pipelines and train/test splits.\n",
    "    - If feature was present during train time but not in the data at predict time\n",
    "      `DictVectorizer` will automatically set the corresponding value to 0.\n",
    "    - If a feature is present at predict time that was not at train time it is not\n",
    "      encoded.\n",
    "2. Features to be encoded are inferred from the data (user does not need to specify this).\n",
    "    - This means numeric values in the input remain unchanged and `str` fields are\n",
    "      encoded automatically.\n",
    "3. We can get a mapping from feature names to the one hot encoded transformation values.\n",
    "    - This is useful for looking at coefficients of feature importances of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cons of DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DictVectorizer` has two main blemishes (as related to a specific but common use case, see disclaimer below).\n",
    "\n",
    "1. Transforming large `DataFrames` is **slow**.\n",
    "2. Transforming large `DataFrames` sometimes results in `MemoryErrors`.\n",
    "2. The `.fit()` and `.transform()` signatures do not accept `DataFrames`. To use `DictVectorizer`\n",
    "    a `DataFrame` must first be converted to a `list` of `dict`s (which is also slow), e.g.\n",
    "    \n",
    "```python\n",
    "    DictVectorizer().fit_transform(X.to_dict('records'))\n",
    "```\n",
    "    \n",
    "Our implementation will guarantee the features of `DictVectorizer` listed in the pros section above and improve the conds by accepting a `DataFrame` as input and vastly improving the speed of the transformation. Our implementation will get a boost in performance by wrapping the super fast `pandas.get_dummies()` with a subclass of `sklearn.base.TransformerMixin`.\n",
    "\n",
    "Before we get started let's compare the speed of `DictVectorizer` with `pandas.get_dummies()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An improved one hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our improved implementation will mimic the `DictVectorizer` interface (except that it accepts `DataFrames` as input) by wrapping the super fast `pandas.get_dummies()` with a subclass of `sklearn.base.TransformerMixin`. Subclassing the `TransformerMixin` makes it easy for our class to integrate with popular `sklearn` paradigms such as their `Pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are specifically comparing the speed of `DictVectorizer` for the following use case only.\n",
    "\n",
    "1. We are starting with a `DataFrame` which must be converted to a list of `dicts`\n",
    "2. We are only interested in dense output, e.g. `DictVectorizer(sparse=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started let's compare the speed of `DictVectorizer` with `pandas.get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create *large* data set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SIZE = 5000000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'int1': np.random.randint(0, 100, size=SIZE),\n",
    "    'int2': np.random.randint(0, 100, size=SIZE),\n",
    "    'float1': np.random.uniform(size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(10)], size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(75)], size=SIZE),\n",
    "    'str1': np.random.choice([str(x) for x in range(150)], size=SIZE),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.36 s, sys: 755 ms, total: 5.11 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `pandas.get_dummies()` is fast. Let's take a look at `DictVectorizers` speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 15.5 s, total: 1min 45s\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "_ = DictVectorizer(sparse=False).fit_transform(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see `pandas.get_dummies()` is *uncomparably* faster. It's also informative to notice that although there is some overhead from calling `to_dict()`, `fit_transform()` is the real bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 997 ms, total: 1min 6s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time just to get list of dicts\n",
    "_ = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "class GetDummies(sklearn.base.TransformerMixin):\n",
    "    \"\"\"Fast one-hot-encoder that makes use of pandas.get_dummies() safely\n",
    "    on train/test splits.\n",
    "    \"\"\"\n",
    "    def __init__(self, dtypes=None):\n",
    "        self.input_columns = None\n",
    "        self.final_columns = None\n",
    "        if dtypes is None:\n",
    "            dtypes = [object, 'category']\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.input_columns = list(X.select_dtypes(self.dtypes).columns)\n",
    "        X = pd.get_dummies(X, columns=self.input_columns)\n",
    "        self.final_columns = X.columns\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        X = pd.get_dummies(X, columns=self.input_columns)\n",
    "        X_columns = X.columns\n",
    "        # if columns in X had values not in the data set used during\n",
    "        # fit add them and set to 0\n",
    "        missing = set(self.final_columns) - set(X_columns)\n",
    "        for c in missing:\n",
    "            X[c] = 0\n",
    "        # remove any new columns that may have resulted from values in\n",
    "        # X that were not in the data set when fit\n",
    "        return X[self.final_columns]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return tuple(self.final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.83 s, sys: 647 ms, total: 9.47 s\n",
      "Wall time: 9.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# let's take a look at its speed\n",
    "get_dummies = GetDummies()\n",
    "get_dummies.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the GetDummies implentation has slowed down a bit from the original `pandas.get_dummes()` due to the overhead of making sure it handles train/test splits correctly, however its still super fast (and that overhead is dependent on the number of columns *not* rows, i.e. we don't have to worry about scaling `GetDummies` to larger `DataFrames`).\n",
    "\n",
    "Let's also take a look at some of its other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('getdummies', <__main__.GetDummies object at 0x7fbd08062dd8>), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GetDummies works in sklearn pipelines too\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(\n",
    "    GetDummies(),\n",
    "    DecisionTreeClassifier(max_depth=3)\n",
    ")\n",
    "model.fit(df.iloc[:100], np.random.choice([0, 1], size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature int2 (0.472777)\n",
      "2. feature float1 (0.270393)\n",
      "3. feature str1_1 (0.141982)\n",
      "4. feature int1 (0.114848)\n",
      "5. feature str1_147 (0.000000)\n",
      "6. feature str1_139 (0.000000)\n",
      "7. feature str1_14 (0.000000)\n",
      "8. feature str1_140 (0.000000)\n",
      "9. feature str1_141 (0.000000)\n",
      "10. feature str1_142 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# you can also pull out the feature names to look at feature importances\n",
    "tree = model.steps[-1][-1]\n",
    "importances = tree.feature_importances_\n",
    "std = np.std(tree.feature_importances_)\n",
    "\n",
    "indices = np.argsort(importances)[:10:-1]\n",
    "feature_names = model.steps[0][-1].get_feature_names()\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(len(feature_names[:10])):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bar_a</th>\n",
       "      <th>bar_c</th>\n",
       "      <th>baz_b</th>\n",
       "      <th>baz_d</th>\n",
       "      <th>grok_hi</th>\n",
       "      <th>grok_there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   foo  bar_a  bar_c  baz_b  baz_d  grok_hi  grok_there\n",
       "0    1      1      0      1      0        1           0\n",
       "1    2      0      1      0      1        0           1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/test splits are safe!\n",
    "get_dummies = GetDummies()\n",
    "\n",
    "# create test data that demonstrates how GetDummies handles\n",
    "# different train/test conditions\n",
    "df1 = pd.DataFrame([\n",
    "    [1, 'a', 'b', 'hi'],\n",
    "    [2, 'c', 'd', 'there']\n",
    "], columns=['foo', 'bar', 'baz', 'grok'])\n",
    "df2 = pd.DataFrame([\n",
    "    [3, 'a', 'e', 'whoa', 0],\n",
    "    [4, 'c', 'b', 'whoa', 0],\n",
    "    [4, 'c', 'b', 'there', 0],\n",
    "], columns=['foo', 'bar', 'baz', 'grok', 'new'])\n",
    "\n",
    "get_dummies.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bar_a</th>\n",
       "      <th>bar_c</th>\n",
       "      <th>baz_b</th>\n",
       "      <th>baz_d</th>\n",
       "      <th>grok_hi</th>\n",
       "      <th>grok_there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   foo  bar_a  bar_c  baz_b  baz_d  grok_hi  grok_there\n",
       "0    3      1      0      0      0        0           0\n",
       "1    4      0      1      1      0        0           0\n",
       "2    4      0      1      1      0        0           1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. the new values of 'e' and 'whoa' are not encoded\n",
    "# 2. features baz_b and baz_d are both set to 0 when no suitable\n",
    "#      value for baz is found\n",
    "get_dummies.transform(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend using the `GetDummies` class as needed only. `sklearn` is a true industry standard and deviations thereof should occur only be when the size of the data necessitates such a change. For most modest sized data sets `DictVectorizer` has served me well. However, as the size of your data scales you may find yourself waiting long periods of time for DV to finish or experience DV spitting out memory errors (in fact, I originally set `SIZE=20000000` in the code above, `get_dummies()` ran in ~90s but DV crashed my kernel twice)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
